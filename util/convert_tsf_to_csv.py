import os
import pandas as pd
from datetime import datetime

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a file .tsf
DATASET_DIR = "/home/myvh07/hoanglmv/Project/llmtime/datasets" 

def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with="NaN", value_column_name="series_value"):
    """
    H√†m ƒë·ªçc file .tsf (ƒë∆∞·ª£c cung c·∫•p b·ªüi Monash Repository)
    """
    col_names = []
    col_types = []
    all_data = {}
    line_count = 0
    frequency = None
    forecast_horizon = None
    contain_missing_values = None
    contain_equal_length = None
    found_data_tag = False
    found_data_section = False
    started_reading_data_section = False

    with open(full_file_path_and_name, 'r', encoding='cp1252') as file:
        for line in file:
            line = line.strip()
            if line:
                if line.startswith("@attribute"):
                    col_names.append(line.split()[1])
                    col_types.append(line.split()[2])
                if line.startswith("@frequency"):
                    frequency = line.split()[1]
                if line.startswith("@horizon"):
                    forecast_horizon = int(line.split()[1])
                if line.startswith("@missing"):
                    contain_missing_values = bool(line.split()[1])
                if line.startswith("@equallength"):
                    contain_equal_length = bool(line.split()[1])

            if not found_data_tag:
                if line.startswith("@data"):
                    found_data_tag = True
            else:
                if line and not started_reading_data_section:
                    started_reading_data_section = True
                    found_data_section = True
                    all_series = []
                    for col in col_names:
                        all_data[col] = []

                if found_data_section:
                    # X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ng d√≤ng
                    parts = line.split(":")
                    if len(parts) >= 2: # ƒê·∫£m b·∫£o c√≥ ph·∫ßn data
                        # Ph·∫ßn metadata (t√™n series, start_timestamp...)
                        meta_part = parts[0].split(",")
                        # Ph·∫ßn gi√° tr·ªã chu·ªói th·ªùi gian
                        series_part = parts[-1].split(",")
                        
                        # Mapping metadata v√†o c·ªôt t∆∞∆°ng ·ª©ng
                        # L∆∞u √Ω: C·∫•u tr√∫c tsf c√≥ th·ªÉ kh√°c nhau s·ªë l∆∞·ª£ng attribute
                        # ƒêo·∫°n n√†y x·ª≠ l√Ω linh ho·∫°t cho c√°c attribute c∆° b·∫£n
                        for idx, val in enumerate(meta_part):
                             if idx < len(col_names) - 1: # Tr·ª´ c·ªôt series_value cu·ªëi c√πng
                                 all_data[col_names[idx]].append(val)
                        
                        # X·ª≠ l√Ω missing values
                        clean_series = []
                        for val in series_part:
                            if val == "?":
                                clean_series.append(replace_missing_vals_with)
                            else:
                                clean_series.append(val)
                        
                        # L∆∞u chu·ªói s·ªë v√†o c·ªôt cu·ªëi c√πng
                        # L∆∞u d∆∞·ªõi d·∫°ng string "val1,val2,..." ƒë·ªÉ CSV kh√¥ng b·ªã v·ª° d√≤ng
                        all_data[col_names[-1]].append(",".join(clean_series))
                        
                    line_count += 1

    if line_count == 0:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu trong file.")
        return None

    # T·∫°o DataFrame
    df = pd.DataFrame(all_data)
    
    # Th√™m th√¥ng tin Frequency v√†o t√™n file ho·∫∑c metadata n·∫øu c·∫ßn
    print(f"   ‚ÑπÔ∏è Frequency: {frequency}, Horizon: {forecast_horizon}")
    
    return df

def process_conversion():
    # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n
    if not os.path.exists(DATASET_DIR):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {DATASET_DIR}")
        return

    print(f"üìÇ ƒêang qu√©t th∆∞ m·ª•c: {DATASET_DIR}")
    
    count = 0
    for root, dirs, files in os.walk(DATASET_DIR):
        for file in files:
            if file.endswith(".tsf"):
                tsf_path = os.path.join(root, file)
                csv_filename = file.replace(".tsf", ".csv")
                csv_path = os.path.join(root, csv_filename)
                
                print(f"\nüîÑ ƒêang x·ª≠ l√Ω: {file}...")
                try:
                    df = convert_tsf_to_dataframe(tsf_path)
                    
                    if df is not None:
                        # L∆∞u ra CSV
                        df.to_csv(csv_path, index=False)
                        print(f"   ‚úÖ ƒê√£ t·∫°o: {csv_filename}")
                        count += 1
                except Exception as e:
                    print(f"   ‚ùå L·ªói khi chuy·ªÉn ƒë·ªïi file {file}: {e}")
                    import traceback
                    traceback.print_exc()

    print(f"\nüéâ HO√ÄN T·∫§T! ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng {count} file.")

if __name__ == "__main__":
    process_conversion()