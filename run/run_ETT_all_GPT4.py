import os
import sys
import pandas as pd
import numpy as np
import pickle
import openai
import time
from dotenv import load_dotenv

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    print("‚ùå L·ªñI: Ch∆∞a t√¨m th·∫•y OPENAI_API_KEY trong file .env")
    sys.exit(1)

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

# --- 2. C·∫§U H√åNH D·ªÆ LI·ªÜU ---
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DATASETS_TO_RUN = {
    "ETTm1": "ETTm1.csv",
    # M·ªü comment c√°c d√≤ng d∆∞·ªõi n·∫øu mu·ªën ch·∫°y ti·∫øp, nh∆∞ng nh·ªõ ki·ªÉm tra t√∫i ti·ªÅn
    # "ETTm2": "ETTm2.csv", 
}

# --- 3. C·∫§U H√åNH MODEL ---
MODEL_NAME = 'gpt-4' 

# C·∫•u h√¨nh "Ti·∫øt ki·ªám"
gpt_hypers = dict(
    temp=0.7,
    alpha=0.9, 
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- 4. H√ÄM L√ÄM S·∫†CH D·ªÆ LI·ªÜU ---
def load_and_clean_data(file_path):
    print(f"   üìñ ƒêang ƒë·ªçc v√† x·ª≠ l√Ω: {file_path}")
    df = pd.read_csv(file_path, low_memory=False)
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if df['date'].isna().sum() > 0: df['date'] = df['date'].ffill().bfill()
    
    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    valid_cols = []
    EPSILON = 1e-5 
    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if df[col].isna().sum() > 0: df[col] = df[col].fillna(EPSILON)
            if (df[col] == 0).sum() > 0: df[col] = df[col].replace(0, EPSILON)
            valid_cols.append(col)
    if 'date' in df.columns: df = df.sort_values(by='date').reset_index(drop=True)
    return df, valid_cols

# --- 5. H√ÄM CH·∫†Y D·ª∞ B√ÅO (TI·∫æT KI·ªÜM & AN TO√ÄN) ---
def run_gpt_datasets():
    print(f"‚ÑπÔ∏è ƒêang ch·∫°y v·ªõi Model OpenAI: {MODEL_NAME}")
    print("üí∞ CH·∫æ ƒê·ªò TI·∫æT KI·ªÜM: Limit=1500, Samples=3")

    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "#"*60)
        print(f"üöÄ DATASET: {ds_name}")
        print("#"*60)
        
        input_path = os.path.join(BASE_DIR, "datasets/ETT-small", file_name)
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}")
        output_file = os.path.join(output_dir, f"results_{ds_name}_{MODEL_NAME}.pkl")
        
        if not os.path.exists(input_path): continue
        os.makedirs(output_dir, exist_ok=True)
        
        df, target_cols = load_and_clean_data(input_path)
        
        # Resume Mode: Load k·∫øt qu·∫£ c≈© ƒë·ªÉ kh√¥ng ch·∫°y l·∫°i t·ªën ti·ªÅn
        if os.path.exists(output_file):
            try:
                with open(output_file, 'rb') as f:
                    ds_results = pickle.load(f)
                print(f"   üìÇ ƒê√£ load {len(ds_results)} c·ªôt t·ª´ file c≈©.")
            except: ds_results = {}
        else:
            ds_results = {}
        
        for col in target_cols:
            if col in ds_results:
                print(f"   ‚è© C·ªôt {col} ƒë√£ c√≥ k·∫øt qu·∫£. B·ªè qua.")
                continue

            print(f"\n--- üîÑ {ds_name} | C·ªôt: {col} ---")
            series = df[col]
            
            # [QUAN TR·ªåNG] C·∫§U H√åNH TI·∫æT KI·ªÜM
            # 1500 ƒëi·ªÉm = kho·∫£ng 2 tu·∫ßn d·ªØ li·ªáu (ETTm1), ƒë·ªß ƒë·ªÉ b·∫Øt chu k√¨
            # ƒê·ª´ng tƒÉng l√™n 3000 n·∫øu mu·ªën ti·∫øt ki·ªám
            limit_size = 1500 
            test_size = 100
            
            if len(series) > limit_size: series = series.iloc[-limit_size:]
            train, test = series.iloc[:-test_size], series.iloc[-test_size:]
            
            # --- RETRY LOOP ---
            max_retries = 10
            for attempt in range(max_retries):
                try:
                    pred_dict = get_llmtime_predictions_data(
                        train, test, 
                        model=MODEL_NAME, 
                        # [QUAN TR·ªåNG] Gi·∫£m samples xu·ªëng 3. 
                        # M·∫∑c ƒë·ªãnh l√† 10-20. S·ªë 3 l√† t·ªëi thi·ªÉu ƒë·ªÉ l·∫•y trung v·ªã (median)
                        num_samples=3, 
                        **gpt_hypers 
                    )
                    
                    ds_results[col] = {
                        'train': train,
                        'test': test,
                        'pred_median': pred_dict['median'],
                        'pred_samples': pred_dict['samples']
                    }
                    print(f"   ‚úÖ Xong c·ªôt {col}")
                    
                    # L∆∞u ngay l·∫≠p t·ª©c
                    with open(output_file, 'wb') as f:
                        pickle.dump(ds_results, f)
                    
                    # [QUAN TR·ªåNG] Ng·ªß 60s ƒë·ªÉ reset Rate Limit v·ªÅ 0
                    print("   üí§ Ngh·ªâ 60s ƒë·ªÉ b·∫£o v·ªá v√≠ ti·ªÅn v√† Rate Limit...")
                    time.sleep(60)
                    break 

                except Exception as e:
                    err_msg = str(e)
                    if "Rate limit" in err_msg:
                        wait_time = 90
                        print(f"   ‚ö†Ô∏è Rate Limit! (Th·ª≠ {attempt+1}/{max_retries}). ƒê·ª£i {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        print(f"   ‚ùå L·ªói c·ªôt {col}: {e}")
                        # L·ªói kh√°c (kh√¥ng ph·∫£i rate limit) th√¨ d·ª´ng ƒë·ªÉ ki·ªÉm tra code
                        break 

    print("\nüéâ HO√ÄN T·∫§T TO√ÄN B·ªò!")

if __name__ == "__main__":
    run_gpt_datasets()