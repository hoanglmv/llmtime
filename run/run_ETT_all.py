import os
import sys
import pandas as pd
import numpy as np
import pickle
import torch
import gc
from dotenv import load_dotenv

# --- 1. Cáº¤U HÃŒNH Há»† THá»NG ---
load_dotenv()
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ['OMP_NUM_THREADS'] = '4'

try:
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token: login(token=hf_token)
except: pass

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import module cá»§a project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

# --- 2. Cáº¤U HÃŒNH Dá»® LIá»†U ---
BASE_DIR = os.path.expanduser("/home/myvh07/hoanglmv/Project/llmtime")

# Danh sÃ¡ch cÃ¡c dataset muá»‘n cháº¡y
DATASETS_TO_RUN = {
    
    "ETTm1": "ETTm1.csv",
    "ETTm2": "ETTm2.csv",
    "ETTh2": "ETTh2.csv",
    "ETTh1": "ETTh1.csv"
}

# Cáº¥u hÃ¬nh Model Llama-7B
llama_hypers = dict(
    temp=0.7,
    alpha=0.95,
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- 3. HÃ€M LÃ€M Sáº CH Dá»® LIá»†U (KHÃ”NG XÃ“A DÃ’NG) ---
def load_and_clean_data(file_path):
    """
    Äá»c file. Tuyá»‡t Ä‘á»‘i KHÃ”NG xÃ³a dÃ²ng nÃ o.
    - Date lá»—i (NaT) -> Äiá»n báº±ng ngÃ y trÆ°á»›c Ä‘Ã³ (ffill).
    - GiÃ¡ trá»‹ lá»—i (NaN/0) -> Äiá»n báº±ng epsilon.
    """
    print(f"   ğŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½: {file_path}")
    
    # low_memory=False Ä‘á»ƒ Ä‘á»c háº¿t file vÃ o RAM
    df = pd.read_csv(file_path, low_memory=False)
    
    # 1. Xá»­ lÃ½ cá»™t date
    if 'date' in df.columns:
        # Chuyá»ƒn Ä‘á»•i sang datetime, lá»—i thÃ nh NaT
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # Äáº¿m lá»—i
        n_date_err = df['date'].isna().sum()
        if n_date_err > 0:
            print(f"      âš ï¸ CÃ³ {n_date_err} dÃ²ng lá»—i ngÃ y thÃ¡ng (NaT). Äang tá»± Ä‘á»™ng Ä‘iá»n (ffill)...")
            # Fill ngÃ y thÃ¡ng báº±ng giÃ¡ trá»‹ cá»§a dÃ²ng trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ khÃ´ng pháº£i xÃ³a dÃ²ng
            df['date'] = df['date'].ffill()
            # Náº¿u dÃ²ng Ä‘áº§u tiÃªn bá»‹ NaT thÃ¬ dÃ¹ng backfill
            df['date'] = df['date'].bfill()
    
    # 2. Xá»­ lÃ½ cÃ¡c cá»™t sá»‘ liá»‡u
    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    valid_cols = []
    EPSILON = 1e-5  # GiÃ¡ trá»‹ nhá» thay tháº¿ cho 0 vÃ  NaN
    
    for col in target_cols:
        if col in df.columns:
            # Ã‰p kiá»ƒu sá»‘, biáº¿n lá»—i (nhÆ° chá»¯ text) thÃ nh NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # --- LOGIC Má»šI: FILL TOÃ€N Bá»˜, KHÃ”NG XÃ“A ---
            # Äáº¿m NaN vÃ  0 Ä‘á»ƒ bÃ¡o cÃ¡o
            count_nan = df[col].isna().sum()
            count_zero = (df[col] == 0).sum()
            
            # Thay tháº¿ NaN báº±ng Epsilon
            if count_nan > 0:
                df[col] = df[col].fillna(EPSILON)
            
            # Thay tháº¿ 0 báº±ng Epsilon
            if count_zero > 0:
                df[col] = df[col].replace(0, EPSILON)
                
            if count_nan > 0 or count_zero > 0:
                print(f"      ğŸ› ï¸  Cá»™t '{col}': ÄÃ£ thay tháº¿ {count_nan} Ã´ NaN vÃ  {count_zero} Ã´ sá»‘ 0 báº±ng {EPSILON}")
            
            valid_cols.append(col)
    
    # 3. Sort láº¡i theo thá»i gian
    # LÆ°u Ã½: VÃ¬ Ä‘Ã£ fill háº¿t NaT nÃªn sort sáº½ á»•n Ä‘á»‹nh
    if 'date' in df.columns:
        df = df.sort_values(by='date').reset_index(drop=True)
    
    print(f"   âœ… Dá»¯ liá»‡u sáºµn sÃ ng: {len(df)} dÃ²ng. CÃ¡c cá»™t há»£p lá»‡: {valid_cols}")
    return df, valid_cols

# --- 4. HÃ€M CHáº Y Dá»° BÃO ---
def run_all_datasets():
    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "#"*60)
        print(f"ğŸš€ Báº®T Äáº¦U Xá»¬ LÃ DATASET: {ds_name}")
        print("#"*60)
        
        # ÄÆ°á»ng dáº«n file
        input_path = os.path.join(BASE_DIR, "datasets/ETT-small", file_name)
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}")
        output_file = os.path.join(output_dir, f"results_{ds_name}.pkl")
        
        if not os.path.exists(input_path):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {input_path}. Bá» qua.")
            continue
            
        os.makedirs(output_dir, exist_ok=True)
        
        # Load dá»¯ liá»‡u (HÃ m má»›i khÃ´ng xÃ³a dÃ²ng)
        df, target_cols = load_and_clean_data(input_path)
        
        # Dictionary lÆ°u káº¿t quáº£ cá»§a dataset nÃ y
        ds_results = {}
        
        # Cháº¡y tá»«ng cá»™t
        for col in target_cols:
            print(f"\n--- ğŸ”„ {ds_name} | Cá»™t: {col} ---")
            
            # --- Dá»ŒN Dáº¸P MEMORY ---
            torch.cuda.empty_cache()
            gc.collect()

            series = df[col]
            
            # Cáº¥u hÃ¬nh split train/test (Láº¥y 2000 dÃ²ng cuá»‘i)
            limit_size = 2000 
            test_size = 100
            
            if len(series) > limit_size:
                series = series.iloc[-limit_size:]
            
            train = series.iloc[:-test_size]
            test = series.iloc[-test_size:]
            
            try:
                # Gá»i Model
                pred_dict = get_llmtime_predictions_data(
                    train, test, 
                    model='llama-7b',
                    num_samples=10,
                    **llama_hypers 
                )
                
                ds_results[col] = {
                    'train': train,
                    'test': test,
                    'pred_median': pred_dict['median'],
                    'pred_samples': pred_dict['samples']
                }
                print(f"   âœ… Xong cá»™t {col}")

            except Exception as e:
                print(f"   âŒ Lá»—i cá»™t {col}: {e}")
                torch.cuda.empty_cache()

        # LÆ°u káº¿t quáº£
        with open(output_file, 'wb') as f:
            pickle.dump(ds_results, f)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ {ds_name} vÃ o: {output_file}")

    print("\nğŸ‰ğŸ‰ğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH! ğŸ‰ğŸ‰ğŸ‰")

if __name__ == "__main__":
    run_all_datasets()