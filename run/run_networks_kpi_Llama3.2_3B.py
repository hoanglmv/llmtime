import os
import sys
import pandas as pd
import numpy as np
import pickle
import torch
import gc
from dotenv import load_dotenv

# --- C·∫§U H√åNH ---
load_dotenv()
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 
os.environ['OMP_NUM_THREADS'] = '4'
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

try:
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token: login(token=hf_token)
except: pass

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

# Dataset KPI
DATASETS_TO_RUN = {
    "networks_kpi": "network_traffic.csv"
}

MODEL_NAME = 'llama-3.2-3b' 

# [QUAN TR·ªåNG] C·∫•u h√¨nh l·∫°i ƒë·ªÉ d·ª± b√°o m∆∞·ª£t m√† h∆°n
llama_hypers = dict(
    temp=0.1,    # <--- GI·∫¢M t·ª´ 0.7 xu·ªëng 0.1: Gi√∫p ƒë∆∞·ªùng d·ª± b√°o ·ªïn ƒë·ªãnh, √≠t rƒÉng c∆∞a
    alpha=0.90,  # <--- GI·∫¢M t·ª´ 0.95 xu·ªëng 0.90: B·ªè qua c√°c gi√° tr·ªã nhi·ªÖu ƒë·ªôt bi·∫øn
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- H√ÄM L√ÄM M∆Ø·ª¢T D·ªÆ LI·ªÜU ---
def smooth_series(series, window_size=5):
    """
    L√†m m∆∞·ª£t d·ªØ li·ªáu b·∫±ng ph∆∞∆°ng ph√°p Rolling Average.
    V·ªõi d·ªØ li·ªáu 30 ph√∫t/ƒëi·ªÉm, window_size=5 s·∫Ω l·∫•y trung b√¨nh trong kho·∫£ng 2.5 gi·ªù.
    ƒêi·ªÅu n√†y gi√∫p lo·∫°i b·ªè c√°c gai nh·ªçn (noise) nh∆∞ng v·∫´n gi·ªØ ƒë∆∞·ª£c xu h∆∞·ªõng ch√≠nh.
    """
    # Rolling mean v·ªõi center=True ƒë·ªÉ kh√¥ng b·ªã l·ªách pha th·ªùi gian
    smoothed = series.rolling(window=window_size, min_periods=1, center=True).mean()
    # L·∫•p ƒë·∫ßy c√°c gi√° tr·ªã NaN ·ªü ƒë·∫ßu/cu·ªëi chu·ªói do rolling t·∫°o ra
    smoothed = smoothed.ffill().bfill()
    return smoothed

def run_network_3b():
    print(f"‚ÑπÔ∏è Network KPI (Smoothed) | Model: {MODEL_NAME}")
    
    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "="*60)
        print(f"üöÄ DATASET: {ds_name}")
        
        input_path = os.path.join(BASE_DIR, "datasets", ds_name, file_name)
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}_{MODEL_NAME}")
        output_file = os.path.join(output_dir, f"results_{ds_name}_{MODEL_NAME}.pkl")
        
        if not os.path.exists(input_path):
            print(f"‚ùå Thi·∫øu data: {input_path}")
            continue
            
        os.makedirs(output_dir, exist_ok=True)
        
        df = pd.read_csv(input_path)
        series = df['value']
        
        # Context: 30 ph√∫t/l·∫ßn -> 48 ƒëi·ªÉm/ng√†y
        # limit_size 2000 ~ 41 ng√†y l·ªãch s·ª≠
        limit_size = 2000 
        test_size = 48  # D·ª± b√°o 2 ng√†y ti·∫øp theo
        
        if len(series) > limit_size:
            series = series.iloc[-limit_size:]
        
        train_raw = series.iloc[:-test_size]
        test = series.iloc[-test_size:]
        
        # [B∆Ø·ªöC QUAN TR·ªåNG] L√ÄM M∆Ø·ª¢T D·ªÆ LI·ªÜU TRAIN TR∆Ø·ªöC KHI ƒê∆ØA V√ÄO MODEL
        print("   üßπ ƒêang l√†m m∆∞·ª£t d·ªØ li·ªáu Train (Smoothing)...")
        train_smoothed = smooth_series(train_raw, window_size=5)

        torch.cuda.empty_cache()
        gc.collect()
        
        try:
            print(f"   ‚è≥ Inference...")
            pred_dict = get_llmtime_predictions_data(
                train_smoothed, # <--- ƒê∆∞a d·ªØ li·ªáu ƒë√£ l√†m m∆∞·ª£t v√†o model
                test, 
                model=MODEL_NAME, 
                num_samples=10, 
                **llama_hypers 
            )
            
            ds_results = {'value': {
                'train': train_raw, # L∆∞u l·∫°i train g·ªëc ƒë·ªÉ v·∫Ω h√¨nh so s√°nh cho ƒë√∫ng th·ª±c t·∫ø
                'train_smoothed': train_smoothed, # L∆∞u th√™m train smooth ƒë·ªÉ debug
                'test': test,
                'pred_median': pred_dict['median'],
                'pred_samples': pred_dict['samples']
            }}
            
            with open(output_file, 'wb') as f:
                pickle.dump(ds_results, f)
            print(f"   ‚úÖ Xong! Saved: {output_file}")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    run_network_3b()