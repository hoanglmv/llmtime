import os
import sys
import pandas as pd
import numpy as np
import pickle
import torch
import gc
from dotenv import load_dotenv

# --- 1. Cáº¤U HÃŒNH Há»† THá»NG ---
load_dotenv()
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 
os.environ['OMP_NUM_THREADS'] = '4'
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

try:
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token: login(token=hf_token)
except: pass

# ThÃªm Ä‘Æ°á»ng dáº«n project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

# --- 2. Cáº¤U HÃŒNH Dá»® LIá»†U ---
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

# TÃªn Folder: TÃªn File CSV
DATASETS_TO_RUN = {
    "sin_wave": "sin_wave.csv" 
}

# --- 3. Cáº¤U HÃŒNH MODEL ---
MODEL_NAME = 'llama-3.2-3b' 

llama_hypers = dict(
    temp=0.7,
    alpha=0.95,
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

def run_sinwave():
    print(f"â„¹ï¸ Äang cháº¡y vá»›i Model Key: {MODEL_NAME}")
    
    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "#"*60)
        print(f"ğŸš€ DATASET: {ds_name}")
        print("#"*60)
        
        # ÄÆ°á»ng dáº«n: datasets/sin_wave/sin_wave.csv
        input_path = os.path.join(BASE_DIR, "datasets", ds_name, file_name)
        
        # Output: output/sin_wave_llama-3.2-3b/
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}_{MODEL_NAME}")
        output_file = os.path.join(output_dir, f"results_{ds_name}_{MODEL_NAME}.pkl")
        
        if not os.path.exists(input_path):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file data: {input_path}")
            print("   ğŸ‘‰ HÃ£y cháº¡y file util/create_sin_dataset.py trÆ°á»›c!")
            continue
            
        os.makedirs(output_dir, exist_ok=True)
        
        # Äá»c dá»¯ liá»‡u
        print(f"   ğŸ“– Äang Ä‘á»c: {input_path}")
        df = pd.read_csv(input_path)
        
        # Chá»‰ cháº¡y cá»™t 'value'
        target_cols = ['value']
        
        ds_results = {}
        
        for col in target_cols:
            if col not in df.columns: continue
            
            print(f"\n--- ğŸ”„ {ds_name} | Cá»™t: {col} ---")
            
            # Dá»n dáº¹p RAM trÆ°á»›c khi cháº¡y
            torch.cuda.empty_cache()
            gc.collect()

            series = df[col]
            
            # Cáº¥u hÃ¬nh: Láº¥y 1000 Ä‘iá»ƒm cuá»‘i Ä‘á»ƒ dá»± bÃ¡o 100 Ä‘iá»ƒm tiáº¿p theo
            limit_size = 1000 
            test_size = 100
            
            if len(series) > limit_size:
                series = series.iloc[-limit_size:]
            
            train = series.iloc[:-test_size]
            test = series.iloc[-test_size:]
            
            try:
                pred_dict = get_llmtime_predictions_data(
                    train, test, 
                    model=MODEL_NAME, 
                    num_samples=10, # Láº¥y 10 máº«u Ä‘á»ƒ váº½ vÃ¹ng tin cáº­y
                    **llama_hypers 
                )
                
                ds_results[col] = {
                    'train': train,
                    'test': test,
                    'pred_median': pred_dict['median'],
                    'pred_samples': pred_dict['samples']
                }
                print(f"   âœ… Xong cá»™t {col}")

                # XÃ³a biáº¿n táº¡m Ä‘á»ƒ giáº£i phÃ³ng RAM ngay
                del pred_dict

            except Exception as e:
                print(f"   âŒ Lá»—i cá»™t {col}: {e}")
                import traceback
                traceback.print_exc()

        with open(output_file, 'wb') as f:
            pickle.dump(ds_results, f)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£: {output_file}")

    print("\nğŸ‰ HOÃ€N Táº¤T!")

if __name__ == "__main__":
    run_sinwave()
