import os
import sys
import pandas as pd
import numpy as np
import pickle
import time
from dotenv import load_dotenv

# --- 1. Cáº¤U HÃŒNH Há»† THá»NG ---
load_dotenv()

# Kiá»ƒm tra API Key
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ Lá»–I: ChÆ°a tÃ¬m tháº¥y OPENAI_API_KEY trong file .env")
    print("ğŸ‘‰ Vui lÃ²ng thÃªm dÃ²ng: OPENAI_API_KEY=sk-proj-...")
    sys.exit(1)

# ThÃªm Ä‘Æ°á»ng dáº«n project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

# --- 2. Cáº¤U HÃŒNH Dá»® LIá»†U ---
BASE_DIR = os.path.expanduser("/home/myvh07/hoanglmv/Project/llmtime")

DATASETS_TO_RUN = {
    "ETTm1": "ETTm1.csv",
    "ETTm2": "ETTm2.csv",
    "ETTh1": "ETTh1.csv",
    "ETTh2": "ETTh2.csv"
}

# --- 3. Cáº¤U HÃŒNH MODEL GPT ---
# LÆ°u Ã½: "text-davinci-003" (GPT-3) Ä‘Ã£ ngá»«ng hoáº¡t Ä‘á»™ng.
# DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch cÃ¡c model thay tháº¿:
MODELS_CONFIG = [
    # Model thay tháº¿ cho GPT-3 (Completion style) - Tá»‘t nháº¥t cho LLMTime
    "gpt-3.5-turbo-instruct", 
    
    # Model GPT-3.5 Chat (CÃ³ thá»ƒ dÃ¹ng nhÆ°ng format prompt cá»§a llmtime tá»‘i Æ°u cho instruct hÆ¡n)
    # "gpt-3.5-turbo", 
]

gpt_hypers = dict(
    temp=0.7,
    alpha=0.95,
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- 4. HÃ€M LÃ€M Sáº CH Dá»® LIá»†U (GIá»® NGUYÃŠN) ---
def load_and_clean_data(file_path):
    """
    Äá»c file. Tuyá»‡t Ä‘á»‘i KHÃ”NG xÃ³a dÃ²ng nÃ o.
    - Date lá»—i (NaT) -> Äiá»n báº±ng ngÃ y trÆ°á»›c Ä‘Ã³ (ffill).
    - GiÃ¡ trá»‹ lá»—i (NaN/0) -> Äiá»n báº±ng epsilon.
    """
    print(f"   ğŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½: {file_path}")
    df = pd.read_csv(file_path, low_memory=False)
    
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        n_date_err = df['date'].isna().sum()
        if n_date_err > 0:
            print(f"      âš ï¸ CÃ³ {n_date_err} dÃ²ng lá»—i ngÃ y thÃ¡ng. Auto-fill...")
            df['date'] = df['date'].ffill().bfill()
    
    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    valid_cols = []
    EPSILON = 1e-5
    
    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            count_nan = df[col].isna().sum()
            count_zero = (df[col] == 0).sum()
            
            if count_nan > 0: df[col] = df[col].fillna(EPSILON)
            if count_zero > 0: df[col] = df[col].replace(0, EPSILON)
                
            if count_nan > 0 or count_zero > 0:
                print(f"      ğŸ› ï¸  Cá»™t '{col}': Sá»­a {count_nan} NaN vÃ  {count_zero} sá»‘ 0.")
            valid_cols.append(col)
    
    if 'date' in df.columns:
        df = df.sort_values(by='date').reset_index(drop=True)
    
    print(f"   âœ… Dá»¯ liá»‡u sáºµn sÃ ng: {len(df)} dÃ²ng.")
    return df, valid_cols

# --- 5. HÃ€M CHáº Y Dá»° BÃO ---
def run_all_datasets_gpt():
    
    # VÃ²ng láº·p qua tá»«ng Model (GPT-3.5, etc.)
    for model_name in MODELS_CONFIG:
        print(f"\n" + "â–ˆ"*60)
        print(f"ğŸ¤– ÄANG CHáº Y MODEL: {model_name}")
        print("â–ˆ"*60)
        
        for ds_name, file_name in DATASETS_TO_RUN.items():
            print(f"\nğŸ‘‰ DATASET: {ds_name}")
            
            input_path = os.path.join(BASE_DIR, "datasets/ETT-small", file_name)
            output_dir = os.path.join(BASE_DIR, f"output/{ds_name}")
            
            # TÃªn file káº¿t quáº£ gáº¯n liá»n vá»›i tÃªn model Ä‘á»ƒ trÃ¡nh ghi Ä‘Ã¨
            safe_model_name = model_name.replace("/", "-")
            output_file = os.path.join(output_dir, f"results_{ds_name}_{safe_model_name}.pkl")
            
            if not os.path.exists(input_path):
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {input_path}")
                continue
            
            os.makedirs(output_dir, exist_ok=True)
            
            # Check náº¿u Ä‘Ã£ cháº¡y rá»“i thÃ¬ bá» qua (Tiáº¿t kiá»‡m tiá»n API)
            if os.path.exists(output_file):
                print(f"   âš ï¸ File káº¿t quáº£ Ä‘Ã£ tá»“n táº¡i: {output_file}. Bá» qua Ä‘á»ƒ tiáº¿t kiá»‡m API.")
                # continue # Bá» comment dÃ²ng nÃ y náº¿u muá»‘n skip file Ä‘Ã£ cháº¡y

            df, target_cols = load_and_clean_data(input_path)
            ds_results = {}
            
            for col in target_cols:
                print(f"   ... Äang dá»± bÃ¡o cá»™t: {col}")
                
                series = df[col]
                limit_size = 2000 
                test_size = 100
                
                if len(series) > limit_size: series = series.iloc[-limit_size:]
                train = series.iloc[:-test_size]
                test = series.iloc[-test_size:]
                
                try:
                    # Gá»i API OpenAI
                    # HÃ m nÃ y sáº½ tá»± nháº­n diá»‡n tÃªn model lÃ  GPT vÃ  dÃ¹ng API thay vÃ¬ load local
                    pred_dict = get_llmtime_predictions_data(
                        train, test, 
                        model=model_name,
                        num_samples=10,
                        **gpt_hypers 
                    )
                    
                    ds_results[col] = {
                        'train': train,
                        'test': test,
                        'pred_median': pred_dict['median'],
                        'pred_samples': pred_dict['samples']
                    }
                    print(f"      âœ… Xong cá»™t {col}")
                    
                    # Nghá»‰ 1 chÃºt Ä‘á»ƒ trÃ¡nh lá»—i Rate Limit cá»§a OpenAI
                    time.sleep(1) 

                except Exception as e:
                    print(f"      âŒ Lá»—i cá»™t {col}: {e}")
                    import traceback
                    traceback.print_exc()

            # LÆ°u káº¿t quáº£
            with open(output_file, 'wb') as f:
                pickle.dump(ds_results, f)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u: {output_file}")

    print("\nğŸ‰ğŸ‰ğŸ‰ HOÃ€N Táº¤T!")

if __name__ == "__main__":
    run_all_datasets_gpt()