import os
import sys
import pandas as pd
import numpy as np
import pickle
import torch
import gc
from dotenv import load_dotenv

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
load_dotenv()
# Ch·ªçn GPU mu·ªën ch·∫°y (0 ho·∫∑c 1 t√πy t√¨nh tr·∫°ng server)
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 
os.environ['OMP_NUM_THREADS'] = '4'
# Ch·ªëng ph√¢n m·∫£nh b·ªô nh·ªõ GPU (R·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh OOM)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

try:
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token: login(token=hf_token)
except: pass

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import module c·ªßa project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data

# --- 2. C·∫§U H√åNH D·ªÆ LI·ªÜU ---
# H√£y ch·∫Øc ch·∫Øn ƒë∆∞·ªùng d·∫´n n√†y ƒë√∫ng tr√™n server c·ªßa b·∫°n
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

# Danh s√°ch c√°c dataset mu·ªën ch·∫°y
DATASETS_TO_RUN = {
    "ETTm1": "ETTm1.csv",
    "ETTm2": "ETTm2.csv",
    "ETTh2": "ETTh2.csv"
}

# --- C·∫§U H√åNH MODEL (LLAMA 3B) ---
# Model Llama 3.2 3B (C·∫ßn update transformers m·ªõi nh·∫•t ƒë·ªÉ ch·∫°y)
MODEL_NAME = 'meta-llama/Llama-3.2-3B' 

llama_hypers = dict(
    temp=0.7,
    alpha=0.95,
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- 3. H√ÄM L√ÄM S·∫†CH D·ªÆ LI·ªÜU (KH√îNG X√ìA D√íNG) ---
def load_and_clean_data(file_path):
    """
    ƒê·ªçc file. Tuy·ªát ƒë·ªëi KH√îNG x√≥a d√≤ng n√†o.
    - Date l·ªói (NaT) -> ƒêi·ªÅn b·∫±ng ng√†y tr∆∞·ªõc ƒë√≥ (ffill).
    - Gi√° tr·ªã l·ªói (NaN/0) -> ƒêi·ªÅn b·∫±ng epsilon.
    """
    print(f"   üìñ ƒêang ƒë·ªçc v√† x·ª≠ l√Ω: {file_path}")
    
    # low_memory=False ƒë·ªÉ ƒë·ªçc h·∫øt file v√†o RAM
    df = pd.read_csv(file_path, low_memory=False)
    
    # 1. X·ª≠ l√Ω c·ªôt date
    if 'date' in df.columns:
        # Chuy·ªÉn ƒë·ªïi sang datetime, l·ªói th√†nh NaT
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        
        # ƒê·∫øm l·ªói
        n_date_err = df['date'].isna().sum()
        if n_date_err > 0:
            print(f"      ‚ö†Ô∏è C√≥ {n_date_err} d√≤ng l·ªói ng√†y th√°ng (NaT). ƒêang t·ª± ƒë·ªông ƒëi·ªÅn (ffill)...")
            # Fill ng√†y th√°ng b·∫±ng gi√° tr·ªã c·ªßa d√≤ng tr∆∞·ªõc ƒë√≥ ƒë·ªÉ kh√¥ng ph·∫£i x√≥a d√≤ng
            df['date'] = df['date'].ffill()
            # N·∫øu d√≤ng ƒë·∫ßu ti√™n b·ªã NaT th√¨ d√πng backfill
            df['date'] = df['date'].bfill()
    
    # 2. X·ª≠ l√Ω c√°c c·ªôt s·ªë li·ªáu
    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    valid_cols = []
    EPSILON = 1e-5  # Gi√° tr·ªã nh·ªè thay th·∫ø cho 0 v√† NaN
    
    for col in target_cols:
        if col in df.columns:
            # √âp ki·ªÉu s·ªë, bi·∫øn l·ªói (nh∆∞ ch·ªØ text) th√†nh NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # --- LOGIC FILL TO√ÄN B·ªò, KH√îNG X√ìA ---
            # ƒê·∫øm NaN v√† 0 ƒë·ªÉ b√°o c√°o
            count_nan = df[col].isna().sum()
            count_zero = (df[col] == 0).sum()
            
            # Thay th·∫ø NaN b·∫±ng Epsilon
            if count_nan > 0:
                df[col] = df[col].fillna(EPSILON)
            
            # Thay th·∫ø 0 b·∫±ng Epsilon
            if count_zero > 0:
                df[col] = df[col].replace(0, EPSILON)
                
            if count_nan > 0 or count_zero > 0:
                print(f"      üõ†Ô∏è  C·ªôt '{col}': ƒê√£ thay th·∫ø {count_nan} √¥ NaN v√† {count_zero} √¥ s·ªë 0 b·∫±ng {EPSILON}")
            
            valid_cols.append(col)
    
    # 3. Sort l·∫°i theo th·ªùi gian
    if 'date' in df.columns:
        df = df.sort_values(by='date').reset_index(drop=True)
    
    print(f"   ‚úÖ D·ªØ li·ªáu s·∫µn s√†ng: {len(df)} d√≤ng. C√°c c·ªôt h·ª£p l·ªá: {valid_cols}")
    return df, valid_cols

# --- 4. H√ÄM CH·∫†Y D·ª∞ B√ÅO ---
def run_all_datasets():
    print(f"‚ÑπÔ∏è ƒêang ch·∫°y v·ªõi Model: {MODEL_NAME}")
    
    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "#"*60)
        print(f"üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù DATASET: {ds_name}")
        print("#"*60)
        
        # ƒê∆∞·ªùng d·∫´n file
        input_path = os.path.join(BASE_DIR, "datasets/ETT-small", file_name)
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}")
        
        # [QUAN TR·ªåNG] ƒê·ªïi t√™n file k·∫øt qu·∫£ ƒë·ªÉ kh√¥ng ƒë√® l√™n file 7B c≈©
        output_file = os.path.join(output_dir, f"results_{ds_name}_Llama3B.pkl")
        
        if not os.path.exists(input_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {input_path}. B·ªè qua.")
            continue
            
        os.makedirs(output_dir, exist_ok=True)
        
        # Load d·ªØ li·ªáu (H√†m m·ªõi kh√¥ng x√≥a d√≤ng)
        df, target_cols = load_and_clean_data(input_path)
        
        # Dictionary l∆∞u k·∫øt qu·∫£ c·ªßa dataset n√†y
        ds_results = {}
        
        # Ch·∫°y t·ª´ng c·ªôt
        for col in target_cols:
            print(f"\n--- üîÑ {ds_name} | C·ªôt: {col} ---")
            
            # --- D·ªåN D·∫∏P MEMORY ---
            torch.cuda.empty_cache()
            gc.collect()

            series = df[col]
            
            # C·∫•u h√¨nh split train/test (L·∫•y 2000 d√≤ng cu·ªëi)
            limit_size = 2000 
            test_size = 100
            
            if len(series) > limit_size:
                series = series.iloc[-limit_size:]
            
            train = series.iloc[:-test_size]
            test = series.iloc[-test_size:]
            
            try:
                # G·ªçi Model 3B
                pred_dict = get_llmtime_predictions_data(
                    train, test, 
                    model=MODEL_NAME,   # <--- D√πng Llama 3B
                    num_samples=10,
                    **llama_hypers 
                )
                
                ds_results[col] = {
                    'train': train,
                    'test': test,
                    'pred_median': pred_dict['median'],
                    'pred_samples': pred_dict['samples']
                }
                print(f"   ‚úÖ Xong c·ªôt {col}")

            except Exception as e:
                print(f"   ‚ùå L·ªói c·ªôt {col}: {e}")
                # In chi ti·∫øt l·ªói ƒë·ªÉ debug n·∫øu transformers ch∆∞a update
                import traceback
                traceback.print_exc()
                torch.cuda.empty_cache()

        # L∆∞u k·∫øt qu·∫£
        with open(output_file, 'wb') as f:
            pickle.dump(ds_results, f)
        print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ {ds_name} (Llama 3B) v√†o: {output_file}")

    print("\nüéâüéâüéâ HO√ÄN T·∫§T TO√ÄN B·ªò QU√Å TR√åNH V·ªöI MODEL 3B! üéâüéâüéâ")

if __name__ == "__main__":
    run_all_datasets()