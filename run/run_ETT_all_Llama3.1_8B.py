import os
import sys
import pandas as pd
import numpy as np
import pickle
import torch
import gc
from functools import partial
from dotenv import load_dotenv

# --- 1. Cáº¤U HÃŒNH Há»† THá»NG ---
load_dotenv()
# Chá»n GPU (0 hoáº·c 1 tÃ¹y server cá»§a báº¡n)
os.environ["CUDA_VISIBLE_DEVICES"] = "0" 
os.environ['OMP_NUM_THREADS'] = '4'
# Chá»‘ng phÃ¢n máº£nh bá»™ nhá»› GPU (Ráº¥t quan trá»ng)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

try:
    from huggingface_hub import login
    hf_token = os.getenv("HF_TOKEN")
    if hf_token: login(token=hf_token)
except: pass

# ThÃªm Ä‘Æ°á»ng dáº«n project
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import cÃ¡c module cá»‘t lÃµi
from data.serialize import SerializerSettings
from models.llmtime import get_llmtime_predictions_data
from models.llms import completion_fns, context_lengths, tokenization_fns
from models.llama import llama_completion_fn, tokenize_fn as llama_tokenize_fn

# ==============================================================================
# ðŸ› ï¸ Cáº¤U HÃŒNH MODEL (RUNTIME INJECTION)
# ==============================================================================
# Táº¡i Ä‘Ã¢y chÃºng ta tá»± Ä‘á»‹nh nghÄ©a model Llama-3.1-8B Ä‘á»ƒ khÃ´ng pháº£i sá»­a file gá»‘c
# ------------------------------------------------------------------------------
# TÃªn model trÃªn HuggingFace (Báº¡n nÃªn dÃ¹ng báº£n 8B nÃ y thay vÃ¬ 3B vÃ¬ nÃ³ thÃ´ng minh hÆ¡n nhiá»u)
REAL_MODEL_PATH = "meta-llama/Meta-Llama-3.1-8B"
# Key Ä‘á»‹nh danh ná»™i bá»™
MY_CUSTOM_KEY = "custom-llama-3.1-8b"

print(f"ðŸ› ï¸ Äang Ä‘Äƒng kÃ½ model: {REAL_MODEL_PATH}...")

# 1. ÄÄƒng kÃ½ hÃ m dá»± Ä‘oÃ¡n
completion_fns[MY_CUSTOM_KEY] = partial(llama_completion_fn, model=REAL_MODEL_PATH)

# 2. ÄÄƒng kÃ½ Ä‘á»™ dÃ i ngá»¯ cáº£nh (Llama 3.1 há»— trá»£ 128k, ta set 16k lÃ  quÃ¡ Ä‘á»§ vÃ  nháº¹)
context_lengths[MY_CUSTOM_KEY] = 16000 

# 3. ÄÄƒng kÃ½ hÃ m tokenize
tokenization_fns[MY_CUSTOM_KEY] = partial(llama_tokenize_fn, model=REAL_MODEL_PATH)

print("âœ… ÄÄƒng kÃ½ thÃ nh cÃ´ng!")

# ==============================================================================

# --- 2. Cáº¤U HÃŒNH Dá»® LIá»†U ---
BASE_DIR = os.path.expanduser("/home/myvh07/hoanglmv/Project/llmtime")

DATASETS_TO_RUN = {
    "ETTm1": "ETTm1.csv",
    "ETTm2": "ETTm2.csv",
    "ETTh1": "ETTh1.csv",
    "ETTh2": "ETTh2.csv"
}

llama_hypers = dict(
    temp=0.7,
    alpha=0.95,
    beta=0.3,
    basic=False,
    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)
)

# --- 3. HÃ€M LÃ€M Sáº CH Dá»® LIá»†U (KHÃ”NG XÃ“A DÃ’NG) ---
def load_and_clean_data(file_path):
    print(f"   ðŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½: {file_path}")
    df = pd.read_csv(file_path, low_memory=False)
    
    # Xá»­ lÃ½ Date
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if df['date'].isna().sum() > 0:
            print(f"      âš ï¸ Sá»­a lá»—i ngÃ y thÃ¡ng (NaT) báº±ng ffill/bfill...")
            df['date'] = df['date'].ffill().bfill()
    
    # Xá»­ lÃ½ Sá»‘ liá»‡u
    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']
    valid_cols = []
    EPSILON = 1e-5
    
    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
            c_nan = df[col].isna().sum()
            c_zero = (df[col] == 0).sum()
            
            if c_nan > 0: df[col] = df[col].fillna(EPSILON)
            if c_zero > 0: df[col] = df[col].replace(0, EPSILON)
                
            if c_nan > 0 or c_zero > 0:
                print(f"      ðŸ› ï¸  Cá»™t '{col}': Sá»­a {c_nan} NaN vÃ  {c_zero} sá»‘ 0.")
            
            valid_cols.append(col)
    
    if 'date' in df.columns:
        df = df.sort_values(by='date').reset_index(drop=True)
    
    print(f"   âœ… Dá»¯ liá»‡u sáºµn sÃ ng: {len(df)} dÃ²ng.")
    return df, valid_cols

# --- 4. HÃ€M CHáº Y Dá»° BÃO ---
def run_all_datasets():
    print(f"\nâ„¹ï¸ Äang cháº¡y vá»›i Model Key: {MY_CUSTOM_KEY}")
    
    for ds_name, file_name in DATASETS_TO_RUN.items():
        print(f"\n" + "#"*60)
        print(f"ðŸš€ DATASET: {ds_name}")
        print("#"*60)
        
        input_path = os.path.join(BASE_DIR, "datasets/ETT-small", file_name)
        output_dir = os.path.join(BASE_DIR, f"output/{ds_name}")
        
        # Äá»•i tÃªn file output Ä‘á»ƒ nháº­n diá»‡n model
        output_file = os.path.join(output_dir, f"results_{ds_name}_Llama3.1-8B.pkl")
        
        if not os.path.exists(input_path):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y: {input_path}")
            continue
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Load dá»¯ liá»‡u
        df, target_cols = load_and_clean_data(input_path)
        ds_results = {}
        
        for col in target_cols:
            print(f"\n--- ðŸ”„ {ds_name} | Cá»™t: {col} ---")
            
            # Dá»n dáº¹p GPU triá»‡t Ä‘á»ƒ
            torch.cuda.empty_cache()
            gc.collect()

            series = df[col]
            
            # Cáº¥u hÃ¬nh Context Window
            # VÃ¬ Llama 3.1 nhá»› tá»‘t, ta cÃ³ thá»ƒ tÄƒng limit_size lÃªn náº¿u muá»‘n (vÃ­ dá»¥ 3000)
            limit_size = 2000 
            test_size = 100
            
            if len(series) > limit_size:
                series = series.iloc[-limit_size:]
            
            train = series.iloc[:-test_size]
            test = series.iloc[-test_size:]
            
            try:
                pred_dict = get_llmtime_predictions_data(
                    train, test, 
                    model=MY_CUSTOM_KEY,   # <--- DÃ¹ng Key tá»± define á»Ÿ trÃªn
                    num_samples=10,
                    **llama_hypers 
                )
                
                ds_results[col] = {
                    'train': train,
                    'test': test,
                    'pred_median': pred_dict['median'],
                    'pred_samples': pred_dict['samples']
                }
                print(f"   âœ… Xong cá»™t {col}")

            except Exception as e:
                print(f"   âŒ Lá»—i cá»™t {col}: {e}")
                import traceback
                traceback.print_exc()
                torch.cuda.empty_cache()

        with open(output_file, 'wb') as f:
            pickle.dump(ds_results, f)
        print(f"\nðŸ’¾ ÄÃ£ lÆ°u: {output_file}")

    print("\nðŸŽ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜!")

if __name__ == "__main__":
    run_all_datasets()