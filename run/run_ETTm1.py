import os

import pandas as pd

import numpy as np

import pickle

import matplotlib.pyplot as plt

from dotenv import load_dotenv



# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---

load_dotenv() 



# √âp d√πng GPU 1

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

os.environ['OMP_NUM_THREADS'] = '4'



try:

    from huggingface_hub import login

    hf_token = os.getenv("HF_TOKEN")

    if hf_token: login(token=hf_token)

except: pass



import sys

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))



from data.serialize import SerializerSettings

from models.llmtime import get_llmtime_predictions_data



BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

# ƒê·∫£m b·∫£o ƒë·ªçc file ƒë√£ l√†m s·∫°ch (cleaned)

DATA_PATH = os.path.join(BASE_DIR, "datasets/ETT-small/ETTm1.csv")

OUTPUT_DIR = os.path.join(BASE_DIR, "output/ETTm1")



os.makedirs(OUTPUT_DIR, exist_ok=True)



# C·∫•u h√¨nh Model (Dictionary ch·ª©a settings)

llama_hypers = dict(

    temp=0.7,

    alpha=0.95,

    beta=0.3,

    basic=False,

    settings=SerializerSettings(base=10, prec=2, signed=True, half_bin_correction=True)

)



def run_ettm1_inference():

    print(f"üöÄ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {DATA_PATH}")

    if not os.path.exists(DATA_PATH):

        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {DATA_PATH}. H√£y ch·∫°y run/preprocess_ETTm1.py tr∆∞·ªõc!")

        return



    # ƒê·ªçc d·ªØ li·ªáu (ƒê√£ s·∫°ch)

    df = pd.read_csv(DATA_PATH)

    

    # √âp ki·ªÉu l·∫°i date ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng l·ªói

    if 'date' in df.columns:

        df['date'] = pd.to_datetime(df['date'])

    

    target_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

    all_results = {}



    for col in target_cols:

        print(f"\n" + "="*50)

        print(f"üîÑ ƒêANG D·ª∞ B√ÅO C·ªòT: {col}")

        print("="*50)



        if col not in df.columns:

            print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt {col} trong d·ªØ li·ªáu.")

            continue



        series = df[col]

        

        # L·∫•y 2000 d√≤ng cu·ªëi

        limit_size = 2000 

        test_size = 100

        

        if len(series) > limit_size:

            series = series.iloc[-limit_size:]

        

        train = series.iloc[:-test_size]

        test = series.iloc[-test_size:]

        

        print(f"   - Train size: {len(train)}")

        print(f"   - Test size:  {len(test)}")



        try:

            # --- [ƒêO·∫†N S·ª¨A L·ªñI QUAN TR·ªåNG] ---

            # Thay v√¨ truy·ªÅn 'model_hypers=...', ta truy·ªÅn th·∫≥ng tham s·ªë v√†o.

            # **llama_hypers s·∫Ω t·ª± ƒë·ªông gi·∫£i n√©n 'settings', 'temp', 'alpha'... v√†o h√†m.

            

            pred_dict = get_llmtime_predictions_data(

                train, test, 

                model='llama-7b',   # <--- Truy·ªÅn t√™n model tr·ª±c ti·∫øp

                num_samples=10,

                **llama_hypers      # <--- Unpack settings v√† c√°c tham s·ªë kh√°c t·ª´ dict

            )

            

            all_results[col] = {

                'train': train,

                'test': test,

                'pred_median': pred_dict['median'],

                'pred_samples': pred_dict['samples']

            }

            print(f"‚úÖ Ho√†n th√†nh d·ª± b√°o c·ªôt {col}")



        except Exception as e:

            print(f"‚ùå L·ªói khi ch·∫°y c·ªôt {col}: {e}")

            import traceback

            traceback.print_exc()



    output_file = os.path.join(OUTPUT_DIR, "results_ETTm1.pkl")

    with open(output_file, 'wb') as f:

        pickle.dump(all_results, f)

    

    print(f"\nüéâ ƒê√É XONG! K·∫øt qu·∫£ l∆∞u t·∫°i: {output_file}")

    print("üëâ H√£y ch·∫°y file util/check_ETTm1.py ƒë·ªÉ xem bi·ªÉu ƒë·ªì.")



if __name__ == "__main__":

    run_ettm1_inference()