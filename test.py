import torch

def check_gpu_memory():
    if not torch.cuda.is_available():
        print("‚ùå Kh√¥ng t√¨m th·∫•y GPU n√†o (CUDA ch∆∞a s·∫µn s√†ng).")
        return

    num_gpus = torch.cuda.device_count()
    print(f"‚úÖ T√¨m th·∫•y {num_gpus} GPU kh·∫£ d·ª•ng v·ªõi PyTorch:\n")

    for i in range(num_gpus):
        # L·∫•y t√™n GPU
        gpu_name = torch.cuda.get_device_name(i)
        
        # L·∫•y th√¥ng tin b·ªô nh·ªõ (tr·∫£ v·ªÅ bytes)
        # free: dung l∆∞·ª£ng c√≤n tr·ªëng
        # total: t·ªïng dung l∆∞·ª£ng
        free, total = torch.cuda.mem_get_info(i)
        
        # ƒê·ªïi ƒë∆°n v·ªã sang GB
        free_gb = free / (1024 ** 3)
        total_gb = total / (1024 ** 3)
        used_gb = total_gb - free_gb
        
        print(f"üîπ GPU {i}: {gpu_name}")
        print(f"   ‚Ä¢ T·ªïng c·ªông : {total_gb:.2f} GB")
        print(f"   ‚Ä¢ ƒêang d√πng : {used_gb:.2f} GB")
        print(f"   ‚Ä¢ C√≤n tr·ªëng : {free_gb:.2f} GB  <-- Quan tr·ªçng nh·∫•t")
        print("-" * 30)

if __name__ == "__main__":
    check_gpu_memory()